{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook contains the code for training the CRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-01 15:41:23,198 INFO Device is cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import timeit\n",
    "import time\n",
    "import warnings\n",
    "import logging as logger\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from dotmap import DotMap\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from convcrf.convcrf import GaussCRF, default_conf\n",
    "from utils.synthetic import augment_label\n",
    "from utils.metrics import Metrics, Averages\n",
    "from demo import do_crf_inference\n",
    "\n",
    "logger.basicConfig(format='%(asctime)s %(levelname)s %(message)s',\n",
    "                    level=logger.INFO,\n",
    "                    stream=sys.stdout)\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "logger.info('Device is {}'.format(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am gonna do a random subsample of size 800 from train set of file list size 1464\n",
      "I am gonna do a random subsample of size 200 from val set of file list size 1449\n",
      "{'weight': 'vector', 'pyinn': False, 'trainable': True, 'norm': 'none', 'pos_feats': {'compat': 3, 'sdims': 3}, 'logsoftmax': True, 'convcomp': False, 'trainable_bias': False, 'blur': 1, 'unary_weight': 1, 'weight_init': 0.2, 'filter_size': 5, 'softmax': True, 'col_feats': {'schan': 13, 'compat': 10, 'sdims': 80, 'use_bias': False}, 'merge': True, 'final_softmax': False}\n",
      "100 400\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
    "from utils.pascal_loader import PascalDatasetLoader\n",
    "\n",
    "path = '/home/jupyter/projects/ConvCRF/datasets/pascal/VOCdevkit/VOC2012'\n",
    "traincrf_dataset = PascalDatasetLoader(path, split='train', sample_size=800)\n",
    "val_dataset = PascalDatasetLoader(path, split='val', sample_size=200)\n",
    "\n",
    "num_classes = traincrf_dataset.num_classes\n",
    "print(default_conf)\n",
    "traincrf_loader = DataLoader(traincrf_dataset, num_workers=8, shuffle=True, batch_size=2)\n",
    "val_loader = DataLoader(val_dataset, num_workers=8, batch_size=2)\n",
    "\n",
    "print(len(val_loader), len(traincrf_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load stored model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = os.path.join(\"/home/jupyter/projects/ConvCRF/datasets\", \"best_model.pkl\")\n",
    "saved_state = torch.load(save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "config = default_conf\n",
    "model = GaussCRF(conf=config, shape=(500, 500), nclasses=num_classes)\n",
    "# model.load_state_dict(saved_state['model_state'])\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the loss function and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion= nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.00005)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-01 15:42:04,909 INFO Starting from iou: 0.9017006073857561\n"
     ]
    }
   ],
   "source": [
    "args = DotMap()\n",
    "args.pyinn = False\n",
    "args.nospeed = False\n",
    "args.output = None\n",
    "\n",
    "running_metrics = Metrics(num_classes)\n",
    "train_loss_avg = Averages()\n",
    "val_loss_avg = Averages()\n",
    "time_avg = Averages()\n",
    "\n",
    "best_iou = saved_state['best_iou']\n",
    "\n",
    "logger.info('Starting from iou: {}'.format(best_iou))\n",
    "\n",
    "num_epochs = 10\n",
    "# lowest_loss = 0.4491"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define method for to run augment_labels on a batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_augment_label(labels):\n",
    "    array = []\n",
    "    for label in labels:\n",
    "#         print(label, label.shape)\n",
    "        unary = augment_label(label, num_classes=num_classes).transpose(2, 0, 1)\n",
    "        array.append(unary)\n",
    "    return np.array(array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the learning rate decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# lambda_lr_decay = lambda epoch: ((1 - (epoch / num_epochs)) ** 0.9) ** 2\n",
    "# scheduler = optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda_lr_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 100] Average loss: 1.3357 Average Time: 2.4956 Learning rate: 5e-05\n",
      "[1, 200] Average loss: 1.1336 Average Time: 2.5140 Learning rate: 5e-05\n",
      "[1, 300] Average loss: 0.9519 Average Time: 2.5141 Learning rate: 5e-05\n",
      "[1, 400] Average loss: 0.8649 Average Time: 2.5228 Learning rate: 5e-05\n",
      "50/100 Loss: 0.8088985270261765: \n",
      "100/100 Loss: 0.8488523012399674: \n",
      "\n",
      " Avg train loss: 1.07155 vs Avg val loss: 0.82888\n",
      "\n",
      "Epoch: 1 Validation Summary\n",
      "Mean Acc : \t 0.8645448463460764\n",
      "Mean IoU : \t 0.7867791268457553\n",
      "FreqW Acc : \t 0.9532814801037066\n",
      "Overall Acc: \t 0.9752038\n",
      "[2, 100] Average loss: 0.8099 Average Time: 2.5093 Learning rate: 5e-05\n",
      "[2, 200] Average loss: 0.9006 Average Time: 2.5230 Learning rate: 5e-05\n",
      "[2, 300] Average loss: 0.8595 Average Time: 2.5196 Learning rate: 5e-05\n",
      "[2, 400] Average loss: 0.7816 Average Time: 2.5138 Learning rate: 5e-05\n",
      "50/100 Loss: 0.7854488557577133: \n",
      "100/100 Loss: 0.8191856533288956: \n",
      "\n",
      " Avg train loss: 0.83792 vs Avg val loss: 0.80232\n",
      "\n",
      "Epoch: 2 Validation Summary\n",
      "Mean Acc : \t 0.8572834023549566\n",
      "Mean IoU : \t 0.7968833321862777\n",
      "FreqW Acc : \t 0.9547813133470354\n",
      "Overall Acc: \t 0.97626158\n",
      "[3, 100] Average loss: 0.8389 Average Time: 2.5177 Learning rate: 5e-05\n",
      "[3, 200] Average loss: 0.7890 Average Time: 2.5211 Learning rate: 5e-05\n",
      "[3, 300] Average loss: 0.8357 Average Time: 2.5287 Learning rate: 5e-05\n",
      "[3, 400] Average loss: 0.8312 Average Time: 2.5093 Learning rate: 5e-05\n",
      "50/100 Loss: 0.7759283170104027: \n",
      "100/100 Loss: 0.8061703902482986: \n",
      "\n",
      " Avg train loss: 0.82371 vs Avg val loss: 0.79105\n",
      "\n",
      "Epoch: 3 Validation Summary\n",
      "Mean Acc : \t 0.8543430756305184\n",
      "Mean IoU : \t 0.7991298328923797\n",
      "FreqW Acc : \t 0.9548598060734836\n",
      "Overall Acc: \t 0.97638218\n",
      "[4, 100] Average loss: 0.8267 Average Time: 2.4941 Learning rate: 5e-05\n",
      "[4, 200] Average loss: 0.7961 Average Time: 2.5128 Learning rate: 5e-05\n",
      "[4, 300] Average loss: 0.8070 Average Time: 2.5063 Learning rate: 5e-05\n",
      "[4, 400] Average loss: 0.8107 Average Time: 2.4906 Learning rate: 5e-05\n",
      "50/100 Loss: 0.7711391568183898: \n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs): \n",
    "    running_loss = 0.0\n",
    "    actual_epoch = epoch + 1\n",
    "#     scheduler.step()\n",
    "    model.train()\n",
    "\n",
    "    for i, (images, labels) in enumerate(traincrf_loader):\n",
    "        iteration = i + 1\n",
    "\n",
    "        start_ts = time.time()\n",
    "\n",
    "        images = images.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "#         labels = labels[0]\n",
    "#         unary = augment_label(labels, num_classes=num_classes)\n",
    "        unary = batch_augment_label(labels)\n",
    "#         unary = unary.transpose(2, 0, 1).reshape([1, num_classes, unary.shape[0], unary.shape[1]])\n",
    "\n",
    "        unary = torch.from_numpy(unary).float().to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        outputs = model(unary=unary, img=images)\n",
    "        \n",
    "        outputs = outputs.transpose(1,2).transpose(2,3).contiguous().view(-1, 21)\n",
    "        labels = labels.view(-1)\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        train_loss_avg.update(loss.item())\n",
    "        time_avg.update(time.time() - start_ts)\n",
    "\n",
    "        if iteration % 100 == 0:\n",
    "            avg_loss = train_loss_avg.avg\n",
    "            print('[{:d}, {:d}] Average loss: {:.4f} Average Time: {:.4f} Learning rate: {}'\n",
    "                  .format(actual_epoch, iteration, avg_loss, time_avg.avg, optimizer.param_groups[0]['lr']))\n",
    "            \n",
    "            train_loss_avg.reset()\n",
    "            time_avg.reset()\n",
    "            \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        val_len = len(val_loader)\n",
    "        running_val_loss = 0.0\n",
    "        for i_val, (images_val, labels_val) in enumerate(val_loader):\n",
    "            iter_val = i_val + 1\n",
    "#             labels_val = labels_val[0] # remove batch dimension\n",
    "#             unary = augment_label(labels_val, num_classes=num_classes)\n",
    "            unary = batch_augment_label(labels_val)\n",
    "\n",
    "#             unary = unary.transpose(2, 0, 1).reshape([1, num_classes, unary.shape[0], unary.shape[1]])\n",
    "            unary = torch.from_numpy(unary).float().to(device)\n",
    "\n",
    "            images_val = images_val.to(device)\n",
    "            labels_val = labels_val.to(device)\n",
    "\n",
    "            predictions = model(unary=unary, img=images_val)\n",
    "            pred = predictions.transpose(1,2).transpose(2,3).contiguous().view(-1, 21)\n",
    "\n",
    "            labels = labels_val.view(-1)\n",
    "            val_loss = criterion(pred, labels)\n",
    "            \n",
    "\n",
    "            preds_np = predictions.data.max(1)[1].cpu().numpy()\n",
    "            labels_np = labels_val.data.cpu().numpy()\n",
    "\n",
    "            running_metrics.update(labels_np, preds_np)\n",
    "            val_loss_avg.update(val_loss.item())\n",
    "            \n",
    "            running_val_loss += val_loss.item()\n",
    "\n",
    "            if iter_val % 50 == 0:\n",
    "                print(\"{}/{} Loss: {}: \".format(iter_val, val_len, val_loss_avg.avg))\n",
    "                val_loss_avg.reset()\n",
    "\n",
    "    score, class_iou = running_metrics.get_scores()\n",
    "    \n",
    "    print('\\n Avg train loss: {:.5f} vs Avg val loss: {:.5f}'\n",
    "          .format(running_loss/len(traincrf_loader), running_val_loss/len(val_loader)))\n",
    "\n",
    "    print('\\nEpoch: {} Validation Summary'.format(actual_epoch))\n",
    "    for k, v in score.items():\n",
    "        print(k, v)\n",
    "    #   writer.add_scalar('val_metrics/{}'.format(k), v, i+1)\n",
    "\n",
    "    running_metrics.reset()\n",
    "\n",
    "    if score[\"Mean IoU : \\t\"] >= best_iou:\n",
    "        best_iou = score[\"Mean IoU : \\t\"]\n",
    "        logger.info('Found new best_iou: {}'.format(best_iou))\n",
    "        state = {\n",
    "                \"epoch\": actual_epoch,\n",
    "                \"model_state\": model.state_dict(),\n",
    "                \"optimizer_state\": optimizer.state_dict(),\n",
    "                \"best_iou\": best_iou,\n",
    "                }\n",
    "        logger.info(save_path)\n",
    "        torch.save(state, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# memory footprint support libraries/code\n",
    "!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n",
    "!pip install gputil\n",
    "!pip install psutil\n",
    "!pip install humanize\n",
    "import psutil\n",
    "import humanize\n",
    "import os\n",
    "import GPUtil as GPU\n",
    "GPUs = GPU.getGPUs()\n",
    "# XXX: only one GPU on Colab and isn’t guaranteed\n",
    "gpu = GPUs[0]\n",
    "def printm():\n",
    " process = psutil.Process(os.getpid())\n",
    " print(\"Gen RAM Free: \" + humanize.naturalsize( psutil.virtual_memory().available ), \" | Proc size: \" + humanize.naturalsize( process.memory_info().rss))\n",
    " print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))\n",
    "printm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gputil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install psutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
