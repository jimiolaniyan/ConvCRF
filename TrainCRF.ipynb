{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook contains the code for training the CRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-15 12:54:11,686 INFO Device is cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import timeit\n",
    "import time\n",
    "import warnings\n",
    "import logging as logger\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from dotmap import DotMap\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from convcrf.convcrf import GaussCRF, default_conf\n",
    "from utils.synthetic import augment_label\n",
    "from utils.metrics import Metrics, Averages\n",
    "from demo import do_crf_inference\n",
    "\n",
    "logger.basicConfig(format='%(asctime)s %(levelname)s %(message)s',\n",
    "                    level=logger.INFO,\n",
    "                    stream=sys.stdout)\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "logger.info('Device is {}'.format(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
    "from utils.pascal_loader import PascalDatasetLoader\n",
    "\n",
    "path = '/home/jupyter/projects/ConvCRF/datasets/pascal/VOCdevkit/VOC2012'\n",
    "traincrf_dataset = PascalDatasetLoader(path, split='traincrf')\n",
    "val_dataset = PascalDatasetLoader(path, split='val')\n",
    "\n",
    "num_classes = traincrf_dataset.num_classes\n",
    "\n",
    "traincrf_loader = DataLoader(traincrf_dataset, num_workers=8, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, num_workers=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load stored model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = os.path.join(\"/home/jupyter/projects/ConvCRF/datasets\", \"best_model.pkl\")\n",
    "saved_state = torch.load(save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussCRF(\n",
       "  (CRF): ConvCRF()\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = default_conf\n",
    "model = GaussCRF(conf=config, shape=(500, 500), nclasses=num_classes)\n",
    "model.load_state_dict(saved_state['model_state'])\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the loss function and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion= nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.00005)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-15 12:56:57,031 INFO Starting from iou: 0.8890707921190808\n"
     ]
    }
   ],
   "source": [
    "args = DotMap()\n",
    "args.pyinn = False\n",
    "args.nospeed = False\n",
    "args.output = None\n",
    "\n",
    "running_metrics = Metrics(num_classes)\n",
    "train_loss_avg = Averages()\n",
    "val_loss_avg = Averages()\n",
    "time_avg = Averages()\n",
    "\n",
    "best_iou = saved_state['best_iou']\n",
    "\n",
    "logger.info('Starting from iou: {}'.format(best_iou))\n",
    "\n",
    "num_epochs = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the learning rate decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# lambda_lr_decay = lambda epoch: ((1 - (epoch / num_epochs)) ** 0.9) ** 2\n",
    "# scheduler = optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda_lr_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 20] Average loss: 0.7369 Average Time: 1.6061 Learning rate: 5e-05\n",
      "[1, 40] Average loss: 0.8456 Average Time: 1.5885 Learning rate: 5e-05\n",
      "[1, 60] Average loss: 0.6376 Average Time: 1.5626 Learning rate: 5e-05\n",
      "[1, 80] Average loss: 0.6051 Average Time: 1.5647 Learning rate: 5e-05\n",
      "[1, 100] Average loss: 0.6747 Average Time: 1.6116 Learning rate: 5e-05\n",
      "[1, 120] Average loss: 0.6719 Average Time: 1.5565 Learning rate: 5e-05\n",
      "[1, 140] Average loss: 0.5067 Average Time: 1.5628 Learning rate: 5e-05\n",
      "[1, 160] Average loss: 0.7389 Average Time: 1.5850 Learning rate: 5e-05\n",
      "[1, 180] Average loss: 0.4766 Average Time: 1.5571 Learning rate: 5e-05\n",
      "[1, 200] Average loss: 0.5366 Average Time: 1.5416 Learning rate: 5e-05\n",
      "[2, 20] Average loss: 0.8146 Average Time: 1.5246 Learning rate: 5e-05\n",
      "[2, 40] Average loss: 0.5297 Average Time: 1.5575 Learning rate: 5e-05\n",
      "[2, 60] Average loss: 0.5168 Average Time: 1.5311 Learning rate: 5e-05\n",
      "[2, 80] Average loss: 0.5479 Average Time: 1.5591 Learning rate: 5e-05\n",
      "[2, 100] Average loss: 0.6791 Average Time: 1.5673 Learning rate: 5e-05\n",
      "[2, 120] Average loss: 0.6114 Average Time: 1.5798 Learning rate: 5e-05\n",
      "[2, 140] Average loss: 0.6664 Average Time: 1.5971 Learning rate: 5e-05\n",
      "[2, 160] Average loss: 0.5156 Average Time: 1.5825 Learning rate: 5e-05\n",
      "[2, 180] Average loss: 0.9390 Average Time: 1.5505 Learning rate: 5e-05\n",
      "[2, 200] Average loss: 0.6726 Average Time: 1.5788 Learning rate: 5e-05\n",
      "[3, 20] Average loss: 0.7867 Average Time: 1.5777 Learning rate: 5e-05\n",
      "[3, 40] Average loss: 0.6901 Average Time: 1.5802 Learning rate: 5e-05\n",
      "[3, 60] Average loss: 0.5411 Average Time: 1.5863 Learning rate: 5e-05\n",
      "[3, 80] Average loss: 0.6015 Average Time: 1.5851 Learning rate: 5e-05\n",
      "[3, 100] Average loss: 0.8432 Average Time: 1.5625 Learning rate: 5e-05\n",
      "[3, 120] Average loss: 0.5020 Average Time: 1.5486 Learning rate: 5e-05\n",
      "[3, 140] Average loss: 0.4841 Average Time: 1.5734 Learning rate: 5e-05\n",
      "[3, 160] Average loss: 0.6368 Average Time: 1.5730 Learning rate: 5e-05\n",
      "[3, 180] Average loss: 0.5833 Average Time: 1.5855 Learning rate: 5e-05\n",
      "[3, 200] Average loss: 0.7039 Average Time: 1.5908 Learning rate: 5e-05\n",
      "[4, 20] Average loss: 0.5260 Average Time: 1.5646 Learning rate: 5e-05\n",
      "[4, 40] Average loss: 0.6034 Average Time: 1.5431 Learning rate: 5e-05\n",
      "[4, 60] Average loss: 0.7560 Average Time: 1.5752 Learning rate: 5e-05\n",
      "[4, 80] Average loss: 0.6091 Average Time: 1.5585 Learning rate: 5e-05\n",
      "[4, 100] Average loss: 0.8515 Average Time: 1.5751 Learning rate: 5e-05\n",
      "[4, 120] Average loss: 0.5865 Average Time: 1.5658 Learning rate: 5e-05\n",
      "[4, 140] Average loss: 0.5007 Average Time: 1.5640 Learning rate: 5e-05\n",
      "[4, 160] Average loss: 0.6687 Average Time: 1.5495 Learning rate: 5e-05\n",
      "[4, 180] Average loss: 0.7164 Average Time: 1.5701 Learning rate: 5e-05\n",
      "[4, 200] Average loss: 0.5705 Average Time: 1.5864 Learning rate: 5e-05\n",
      "[5, 20] Average loss: 0.7828 Average Time: 1.5561 Learning rate: 5e-05\n",
      "[5, 40] Average loss: 0.8517 Average Time: 1.5731 Learning rate: 5e-05\n",
      "[5, 60] Average loss: 0.4886 Average Time: 1.5705 Learning rate: 5e-05\n",
      "[5, 80] Average loss: 0.6517 Average Time: 1.5978 Learning rate: 5e-05\n",
      "[5, 100] Average loss: 0.5696 Average Time: 1.5646 Learning rate: 5e-05\n",
      "[5, 120] Average loss: 0.5993 Average Time: 1.5954 Learning rate: 5e-05\n",
      "[5, 140] Average loss: 0.6640 Average Time: 1.5804 Learning rate: 5e-05\n",
      "[5, 160] Average loss: 0.6481 Average Time: 1.5610 Learning rate: 5e-05\n",
      "[5, 180] Average loss: 0.6256 Average Time: 1.5878 Learning rate: 5e-05\n",
      "[5, 200] Average loss: 0.3988 Average Time: 1.6023 Learning rate: 5e-05\n",
      "[6, 20] Average loss: 0.7592 Average Time: 1.5910 Learning rate: 5e-05\n",
      "[6, 40] Average loss: 0.5391 Average Time: 1.5878 Learning rate: 5e-05\n",
      "[6, 60] Average loss: 0.5373 Average Time: 1.5808 Learning rate: 5e-05\n",
      "[6, 80] Average loss: 0.5076 Average Time: 1.5837 Learning rate: 5e-05\n",
      "[6, 100] Average loss: 0.5584 Average Time: 1.5932 Learning rate: 5e-05\n",
      "[6, 120] Average loss: 0.8284 Average Time: 1.5976 Learning rate: 5e-05\n",
      "[6, 140] Average loss: 0.6184 Average Time: 1.5503 Learning rate: 5e-05\n",
      "[6, 160] Average loss: 0.5821 Average Time: 1.5917 Learning rate: 5e-05\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs): \n",
    "    running_loss = 0.0\n",
    "    actual_epoch = epoch + 1\n",
    "#     scheduler.step()\n",
    "    model.train()\n",
    "\n",
    "    for i, data in enumerate(traincrf_loader):\n",
    "        iteration = i + 1\n",
    "        \n",
    "        start_ts = time.time()\n",
    "        images, labels = data\n",
    "\n",
    "        images = images.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        labels = labels[0]\n",
    "        unary = augment_label(labels, num_classes=num_classes)\n",
    "        unary = unary.transpose(2, 0, 1).reshape([1, num_classes, unary.shape[0], unary.shape[1]])\n",
    "\n",
    "        unary = torch.from_numpy(unary).float().to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = model(unary=unary, img=images)\n",
    "        \n",
    "        outputs = outputs.transpose(1,2).transpose(2,3).contiguous().view(-1, 21)\n",
    "        labels = labels.view(-1)\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss_avg.update(loss.item())\n",
    "        time_avg.update(time.time() - start_ts)\n",
    "\n",
    "        if iteration % 20 == 0:\n",
    "            print('[{:d}, {:d}] Average loss: {:.4f} Average Time: {:.4f} Learning rate: {}'\n",
    "                  .format(actual_epoch, iteration, train_loss_avg.avg, time_avg.avg, optimizer.param_groups[0]['lr']))\n",
    "            \n",
    "            train_loss_avg.reset()\n",
    "            time_avg.reset()\n",
    "            \n",
    "        if actual_epoch % 10 == 0 and iteration == 200:\n",
    "            logger.info('Doing validation at Epoch: {}'.format(actual_epoch))\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                val_len = len(val_loader)\n",
    "                for i_val, (images_val, labels_val) in enumerate(val_loader):\n",
    "                    iter_val = i_val + 1\n",
    "                    labels_val = labels_val[0] # remove batch dimension\n",
    "                    unary = augment_label(labels_val, num_classes=num_classes)\n",
    "                    \n",
    "                    unary = unary.transpose(2, 0, 1).reshape([1, num_classes, unary.shape[0], unary.shape[1]])\n",
    "                    unary = torch.from_numpy(unary).float().to(device)\n",
    "                    \n",
    "                    images_val = images_val.to(device)\n",
    "                    labels_val = labels_val.to(device)\n",
    "                    \n",
    "                    predictions = model(unary=unary, img=images_val)\n",
    "                    pred = predictions.transpose(1,2).transpose(2,3).contiguous().view(-1, 21)\n",
    "                    \n",
    "                    labels = labels_val.view(-1)\n",
    "                    val_loss = criterion(pred, labels)\n",
    "                    \n",
    "                    preds_np = predictions.data.max(1)[1].cpu().numpy()[0]\n",
    "                    labels_np = labels_val.data.cpu().numpy()\n",
    "                    \n",
    "                    running_metrics.update(labels_np, preds_np)\n",
    "                    val_loss_avg.update(val_loss.item())\n",
    "                    \n",
    "                    if iter_val % 200 == 0:\n",
    "                        print(\"{}/{} Loss: {}: \".format(iter_val, val_len, val_loss_avg.avg))\n",
    "                        val_loss_avg.reset()\n",
    "                \n",
    "            logger.info(\"Epoch %d Loss: %.4f\" % (actual_epoch, val_loss_avg.avg))\n",
    "            score, class_iou = running_metrics.get_scores()\n",
    "            \n",
    "            print('\\nEpoch: {} Validation Suammry'.format(actual_epoch))\n",
    "            for k, v in score.items():\n",
    "                print(k, v)\n",
    "            #   writer.add_scalar('val_metrics/{}'.format(k), v, i+1)\n",
    "        \n",
    "            running_metrics.reset()\n",
    "        \n",
    "            if score[\"Mean IoU : \\t\"] >= best_iou:\n",
    "                best_iou = score[\"Mean IoU : \\t\"]\n",
    "                logger.info('Found new best_iou: {}'.format(best_iou))\n",
    "                state = {\n",
    "                        \"epoch\": actual_epoch,\n",
    "                        \"model_state\": model.state_dict(),\n",
    "                        \"optimizer_state\": optimizer.state_dict(),\n",
    "                        \"best_iou\": best_iou,\n",
    "                        }\n",
    "                logger.info(save_path)\n",
    "                torch.save(state, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
