{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"TrainCRF_Deeplab.ipynb","version":"0.3.2","provenance":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"Ubtqx5ST6Qi-","colab_type":"text"},"cell_type":"markdown","source":["# Colab formalities"]},{"metadata":{"id":"MUKatAxI62ek","colab_type":"text"},"cell_type":"markdown","source":["## Connect to gDrive"]},{"metadata":{"id":"mKVcWHZq6PbB","colab_type":"code","colab":{}},"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive/')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"G-2wEEJZ66OF","colab_type":"text"},"cell_type":"markdown","source":["## Download PyTorch"]},{"metadata":{"id":"mCx7jhyZ60rL","colab_type":"code","colab":{}},"cell_type":"code","source":["from os.path import exists\n","from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n","platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n","cuda_output = !ldconfig -p|grep cudart.so|sed -e 's/.*\\.\\([0-9]*\\)\\.\\([0-9]*\\)$/cu\\1\\2/'\n","accelerator = cuda_output[0] if exists('/dev/nvidia0') else 'cpu'\n","\n","!pip install -q http://download.pytorch.org/whl/{accelerator}/torch-0.4.1-{platform}-linux_x86_64.whl torchvision tensorboardX"],"execution_count":0,"outputs":[]},{"metadata":{"id":"vgUZljlt7AQH","colab_type":"text"},"cell_type":"markdown","source":["## Change to ConvCRF directory"]},{"metadata":{"id":"EO8oTkuU7FGk","colab_type":"code","colab":{}},"cell_type":"code","source":["import os\n","os.chdir('/content/drive/My Drive/colab/ConvCRF')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"YSxb95Oo8bgW","colab_type":"text"},"cell_type":"markdown","source":["## In case of PIL error"]},{"metadata":{"id":"y4CDwNIa8eYi","colab_type":"code","outputId":"66be0fd4-e819-4df8-e959-cd17aff32e55","executionInfo":{"status":"ok","timestamp":1546880375931,"user_tz":-60,"elapsed":1459,"user":{"displayName":"Folajimi OLANIYAN","photoUrl":"","userId":"08647342417096594225"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"cell_type":"code","source":["# ! pip uninstall -y pillow\n","# ! pip install pillow==5.3.0\n","# import PIL\n","# PIL.__version__"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'5.3.0'"]},"metadata":{"tags":[]},"execution_count":1}]},{"metadata":{"id":"ZxVmMl5x6bTj","colab_type":"text"},"cell_type":"markdown","source":["# Train CRF from Deeplab Unary"]},{"metadata":{"id":"md5qs6OT69xa","colab_type":"code","colab":{}},"cell_type":"code","source":["%matplotlib inline\n","\n","%load_ext autoreload\n","%autoreload 2"],"execution_count":0,"outputs":[]},{"metadata":{"id":"A_CpuiGJ7dkB","colab_type":"code","colab":{}},"cell_type":"code","source":["! pip install dotmap"],"execution_count":0,"outputs":[]},{"metadata":{"id":"rKHKthfS7QLm","colab_type":"code","colab":{}},"cell_type":"code","source":["import os\n","import sys\n","import cv2\n","import scipy\n","import timeit\n","import time\n","import warnings\n","import logging as logger\n","import matplotlib.pyplot as plt\n","import numpy as np\n","from tqdm import tqdm\n","from dotmap import DotMap\n","from PIL import Image\n","\n","import torch\n","import torch.nn as nn\n","import torchvision.models as models\n","from torchvision import transforms\n","\n","from convcrf.convcrf import GaussCRF, default_conf\n","from utils.metrics import Metrics, Averages\n","from demo import do_crf_inference\n","\n","logger.basicConfig(format='%(asctime)s %(levelname)s %(message)s',\n","                    level=logger.INFO,\n","                    stream=sys.stdout)\n","\n","warnings.filterwarnings('ignore')\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","logger.info('Device is {}'.format(device))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"dfkxOCSn6F4T","colab_type":"text"},"cell_type":"markdown","source":["## Load Data"]},{"metadata":{"id":"j62rMMnz6F4V","colab_type":"code","colab":{}},"cell_type":"code","source":["from torch.utils.data import DataLoader, WeightedRandomSampler\n","from utils.pascal_loader_old import PascalDatasetLoader\n","\n","path = '/content/drive/My Drive/Research/Datasets/VOCdevkit/VOC2012'\n","traincrf_dataset = PascalDatasetLoader(path, split='train', img_size=513)\n","val_dataset = PascalDatasetLoader(path, split='val', img_size=513)\n","\n","num_classes = 21\n","traincrf_loader = DataLoader(traincrf_dataset, num_workers=4, shuffle=True, batch_size=1)\n","val_loader = DataLoader(val_dataset, num_workers=4, batch_size=1)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"gaChvXyypg73","colab_type":"code","colab":{}},"cell_type":"code","source":["list(range(10))"],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"text","id":"0YO3c-SQi7-P"},"cell_type":"markdown","source":["## Load the image and normalize"]},{"metadata":{"id":"ynFF19W_HOCZ","colab_type":"code","colab":{}},"cell_type":"code","source":["import cv2\n","def load_image_tensor(img_path):\n","\n","  img = np.zeros((513,513,3));\n","#   img[:,:,::-1]\n","  img_temp = cv2.imread(img_path).astype(float)\n","  img_original = img_temp\n","  img_temp[:,:,0] = img_temp[:,:,0] - 104.008\n","  img_temp[:,:,1] = img_temp[:,:,1] - 116.669\n","  img_temp[:,:,2] = img_temp[:,:,2] - 122.675\n","  img[:img_temp.shape[0],:img_temp.shape[1],:] = img_temp\n","  return torch.from_numpy(img[np.newaxis, :].transpose(0,3,1,2)).float()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"I_4zv-idLye1","colab_type":"text"},"cell_type":"markdown","source":["## Define Deeplab model"]},{"metadata":{"id":"4in_8v8QLwS5","colab_type":"code","colab":{}},"cell_type":"code","source":["import torch.nn as nn\n","import math\n","import torch.utils.model_zoo as model_zoo\n","import torch\n","import numpy as np\n","affine_par = True\n","\n","\n","def outS(i):\n","    i = int(i)\n","    i = (i+1)/2\n","    i = int(np.ceil((i+1)/2.0))\n","    i = (i+1)//2\n","    return i\n","def conv3x3(in_planes, out_planes, stride=1):\n","    \"3x3 convolution with padding\"\n","    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n","                     padding=1, bias=False)\n","\n","\n","class BasicBlock(nn.Module):\n","    expansion = 1\n","\n","    def __init__(self, inplanes, planes, stride=1, downsample=None):\n","        super(BasicBlock, self).__init__()\n","        self.conv1 = conv3x3(inplanes, planes, stride)\n","        self.bn1 = nn.BatchNorm2d(planes, affine = affine_par)\n","        self.relu = nn.ReLU(inplace=True)\n","        self.conv2 = conv3x3(planes, planes)\n","        self.bn2 = nn.BatchNorm2d(planes, affine = affine_par)\n","        self.downsample = downsample\n","        self.stride = stride\n","\n","    def forward(self, x):\n","        residual = x\n","\n","        out = self.conv1(x)\n","        out = self.bn1(out)\n","        out = self.relu(out)\n","\n","        out = self.conv2(out)\n","        out = self.bn2(out)\n","\n","        if self.downsample is not None:\n","            residual = self.downsample(x)\n","\n","        out += residual\n","        out = self.relu(out)\n","\n","        return out\n","\n","\n","class Bottleneck(nn.Module):\n","    expansion = 4\n","\n","    def __init__(self, inplanes, planes, stride=1,  dilation_ = 1, downsample=None):\n","        super(Bottleneck, self).__init__()\n","        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, stride=stride, bias=False) # change\n","        self.bn1 = nn.BatchNorm2d(planes,affine = affine_par)\n","        \n","        for i in self.bn1.parameters():\n","          i.requires_grad = False\n","        padding = 1\n","        if dilation_ == 2:\n","\t        padding = 2\n","        elif dilation_ == 4:\n","\t        padding = 4\n","        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, # change\n","                               padding=padding, bias=False, dilation = dilation_)\n","        self.bn2 = nn.BatchNorm2d(planes,affine = affine_par)\n","        for i in self.bn2.parameters():\n","            i.requires_grad = False\n","        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\n","        self.bn3 = nn.BatchNorm2d(planes * 4, affine = affine_par)\n","        for i in self.bn3.parameters():\n","            i.requires_grad = False\n","        self.relu = nn.ReLU(inplace=True)\n","        self.downsample = downsample\n","        self.stride = stride\n","\n","\n","\n","    def forward(self, x):\n","        residual = x\n","\n","        out = self.conv1(x)\n","        out = self.bn1(out)\n","        out = self.relu(out)\n","\n","        out = self.conv2(out)\n","        out = self.bn2(out)\n","        out = self.relu(out)\n","\n","        out = self.conv3(out)\n","        out = self.bn3(out)\n","\n","        if self.downsample is not None:\n","            residual = self.downsample(x)\n","\n","        out += residual\n","        out = self.relu(out)\n","        return out\n","\n","class Classifier_Module(nn.Module):\n","\n","    def __init__(self,dilation_series,padding_series,NoLabels):\n","        super(Classifier_Module, self).__init__()\n","        self.conv2d_list = nn.ModuleList()\n","        for dilation,padding in zip(dilation_series,padding_series):\n","    \t    self.conv2d_list.append(nn.Conv2d(2048,NoLabels,kernel_size=3,stride=1, padding =padding, dilation = dilation,bias = True))\n","\n","        for m in self.conv2d_list:\n","          m.weight.data.normal_(0, 0.01)\n","\n","\n","    def forward(self, x):\n","      out = self.conv2d_list[0](x)\n","      for i in range(len(self.conv2d_list)-1):\n","        out += self.conv2d_list[i+1](x)\n","      return out\n","\n","class ResNet(nn.Module):\n","    def __init__(self, block, layers,NoLabels):\n","        self.inplanes = 64\n","        super(ResNet, self).__init__()\n","        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3,\n","                               bias=False)\n","        self.bn1 = nn.BatchNorm2d(64,affine = affine_par)\n","        for i in self.bn1.parameters():\n","            i.requires_grad = False\n","        self.relu = nn.ReLU(inplace=True)\n","        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1, ceil_mode=True) # change\n","        self.layer1 = self._make_layer(block, 64, layers[0])\n","        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n","        self.layer3 = self._make_layer(block, 256, layers[2], stride=1, dilation__ = 2)\n","        self.layer4 = self._make_layer(block, 512, layers[3], stride=1, dilation__ = 4)\n","        self.layer5 = self._make_pred_layer(Classifier_Module, [6,12,18,24],[6,12,18,24],NoLabels)\n","\n","        for m in self.modules():\n","            if isinstance(m, nn.Conv2d):\n","                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n","                m.weight.data.normal_(0, 0.01)\n","            elif isinstance(m, nn.BatchNorm2d):\n","                m.weight.data.fill_(1)\n","                m.bias.data.zero_()\n","        #        for i in m.parameters():\n","        #            i.requires_grad = False\n","\n","    def _make_layer(self, block, planes, blocks, stride=1,dilation__ = 1):\n","        downsample = None\n","        if stride != 1 or self.inplanes != planes * block.expansion or dilation__ == 2 or dilation__ == 4:\n","            downsample = nn.Sequential(\n","                nn.Conv2d(self.inplanes, planes * block.expansion,\n","                          kernel_size=1, stride=stride, bias=False),\n","                nn.BatchNorm2d(planes * block.expansion,affine = affine_par),\n","            )\n","        for i in downsample._modules['1'].parameters():\n","            i.requires_grad = False\n","        layers = []\n","        layers.append(block(self.inplanes, planes, stride,dilation_=dilation__, downsample = downsample ))\n","        self.inplanes = planes * block.expansion\n","        for i in range(1, blocks):\n","            layers.append(block(self.inplanes, planes,dilation_=dilation__))\n","\n","        return nn.Sequential(*layers)\n","        \n","    def _make_pred_layer(self,block, dilation_series, padding_series,NoLabels):\n","\t    return block(dilation_series,padding_series,NoLabels)\n","\n","    def forward(self, x):\n","        x = self.conv1(x)\n","        x = self.bn1(x)\n","        x = self.relu(x)\n","        x = self.maxpool(x)\n","        x = self.layer1(x)\n","        x = self.layer2(x)\n","        x = self.layer3(x)\n","        x = self.layer4(x)\n","        x = self.layer5(x)\n","\n","        return x\n","\n","class MS_Deeplab(nn.Module):\n","    def __init__(self,block,NoLabels):\n","    \tsuper(MS_Deeplab,self).__init__()\n","    \tself.Scale = ResNet(block,[3, 4, 23, 3],NoLabels)   #changed to fix #4 \n","\n","    def forward(self,x):\n","        input_size = x.size()[2]\n","        self.interp1 = nn.UpsamplingBilinear2d(size=(int(input_size*0.75)+1, int(input_size*0.75)+1))\n","        self.interp2 = nn.UpsamplingBilinear2d(size=(int(input_size*0.5)+1, int(input_size*0.5)+1))\n","        self.interp3 = nn.UpsamplingBilinear2d(size=(outS(input_size), outS(input_size)))\n","        out = []\n","        x2 = self.interp1(x)\n","        x3 = self.interp2(x)\n","        out.append(self.Scale(x))\t# for original scale\n","        out.append(self.interp3(self.Scale(x2)))\t# for 0.75x scale\n","        out.append(self.Scale(x3))\t# for 0.5x scale\n","\n","\n","        x2Out_interp = out[1]\n","        x3Out_interp = self.interp3(out[2])\n","        temp1 = torch.max(out[0],x2Out_interp)\n","        out.append(torch.max(temp1,x3Out_interp))\n","        return out\n","\n","def Res_Deeplab(NoLabels=21):\n","    model = MS_Deeplab(Bottleneck,NoLabels)\n","    return model"],"execution_count":0,"outputs":[]},{"metadata":{"id":"nr09abUw_lu7","colab_type":"text"},"cell_type":"markdown","source":["## Load deeplab weights\n"]},{"metadata":{"id":"XS0s7lOk_lI3","colab_type":"code","colab":{}},"cell_type":"code","source":["model_path = '/content/drive/My Drive/Research/Models/Pretrained/MS_DeepLab_resnet_trained_VOC.pth'\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","deeplab_model = Res_Deeplab()\n","deeplab_model.load_state_dict(torch.load(model_path))\n","\n","deeplab_model = deeplab_model.to(device)\n","deeplab_model.eval()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"i1c1dgQtMo3d","colab_type":"text"},"cell_type":"markdown","source":["## Get the Unary from Deeplab output"]},{"metadata":{"id":"L9-boG8HMXve","colab_type":"code","colab":{}},"cell_type":"code","source":["def get_unary(outputs):\n","  interp = nn.UpsamplingBilinear2d(size=(513,513))\n","  output = interp(outputs[3]).cpu().data[0].numpy()\n","  output = output.transpose(1,2,0)\n","  output = np.argmax(output,axis = 2)\n","  return output\n","#   return output[None, :, :, :]"],"execution_count":0,"outputs":[]},{"metadata":{"id":"mDqIlV9w6F4l","colab_type":"text"},"cell_type":"markdown","source":["## Define the CRF model"]},{"metadata":{"id":"ehLX_O706F4o","colab_type":"code","colab":{}},"cell_type":"code","source":["config = default_conf\n","crf_model = GaussCRF(conf=config, shape=(500, 500), nclasses=num_classes)\n","# model.load_state_dict(saved_state['model_state'])\n","crf_model = crf_model.to(device)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"I-X8_46S6F4t","colab_type":"text"},"cell_type":"markdown","source":["## Define the loss function and optimizer"]},{"metadata":{"id":"6GeGCArZ6F4v","colab_type":"code","colab":{}},"cell_type":"code","source":["import torch.optim as optim\n","\n","criterion= nn.CrossEntropyLoss()\n","optimizer = optim.Adam(crf_model.parameters(), lr=0.00005)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"R_QPjM-l6F42","colab_type":"text"},"cell_type":"markdown","source":["## Train the network"]},{"metadata":{"id":"OVske0zc6F45","colab_type":"code","colab":{}},"cell_type":"code","source":["args = DotMap()\n","args.pyinn = False\n","args.nospeed = False\n","args.output = None\n","\n","running_metrics = Metrics(num_classes)\n","train_loss_avg = Averages()\n","val_loss_avg = Averages()\n","time_avg = Averages()\n","\n","# best_iou = saved_state['best_iou']\n","\n","# logger.info('Starting from iou: {}'.format(best_iou))\n","\n","num_epochs = 10"],"execution_count":0,"outputs":[]},{"metadata":{"id":"3-RWoPTdOHV-","colab_type":"code","outputId":"f05e1756-b7f5-4ffe-c885-3691a2ce890b","executionInfo":{"status":"ok","timestamp":1546882181827,"user_tz":-60,"elapsed":1390,"user":{"displayName":"Folajimi OLANIYAN","photoUrl":"","userId":"08647342417096594225"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"cell_type":"code","source":["images, labels = next(iter(traincrf_loader))\n","img = images.to(device)\n","outputs = deeplab_model(img)\n","o = get_unary(outputs)\n","np.min(o)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0"]},"metadata":{"tags":[]},"execution_count":37}]},{"metadata":{"id":"53qr3ZPXWJsP","colab_type":"code","colab":{}},"cell_type":"code","source":["from tensorboardX import SummaryWriter\n","writer = SummaryWriter"],"execution_count":0,"outputs":[]},{"metadata":{"id":"MDC_1nSjOBrV","colab_type":"code","colab":{}},"cell_type":"code","source":["for epoch in range(1): \n","    running_loss = 0.0\n","    actual_epoch = epoch + 1\n","#     scheduler.step()\n","    crf_model.train()\n","\n","    for i, (images, labels) in enumerate(traincrf_loader):\n","        iteration = i + 1\n","\n","        start_ts = time.time()\n","\n","        images = images.to(device)\n","\n","        optimizer.zero_grad()\n","\n","#         labels = labels[0]\n","#         unary = augment_label(labels, num_classes=num_classes)\n","        unary = batch_augment_label(labels)\n","#         unary = unary.transpose(2, 0, 1).reshape([1, num_classes, unary.shape[0], unary.shape[1]])\n","\n","        unary = torch.from_numpy(unary).float().to(device)\n","        labels = labels.to(device)\n","        \n","        outputs = model(unary=unary, img=images)\n","        \n","        outputs = outputs.transpose(1,2).transpose(2,3).contiguous().view(-1, 21)\n","        labels = labels.view(-1)\n","\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","        \n","        running_loss += loss.item()\n","        train_loss_avg.update(loss.item())\n","        time_avg.update(time.time() - start_ts)\n","\n","        if iteration % 100 == 0:\n","            avg_loss = train_loss_avg.avg\n","            print('[{:d}, {:d}] Average loss: {:.4f} Average Time: {:.4f} Learning rate: {}'\n","                  .format(actual_epoch, iteration, avg_loss, time_avg.avg, optimizer.param_groups[0]['lr']))\n","            \n","            train_loss_avg.reset()\n","            time_avg.reset()\n","            \n","    \n","    with torch.no_grad():\n","        model.eval()\n","        val_len = len(val_loader)\n","        running_val_loss = 0.0\n","        for i_val, (images_val, labels_val) in enumerate(val_loader):\n","            iter_val = i_val + 1\n","#             labels_val = labels_val[0] # remove batch dimension\n","#             unary = augment_label(labels_val, num_classes=num_classes)\n","            unary = batch_augment_label(labels_val)\n","\n","#             unary = unary.transpose(2, 0, 1).reshape([1, num_classes, unary.shape[0], unary.shape[1]])\n","            unary = torch.from_numpy(unary).float().to(device)\n","\n","            images_val = images_val.to(device)\n","            labels_val = labels_val.to(device)\n","\n","            predictions = model(unary=unary, img=images_val)\n","            pred = predictions.transpose(1,2).transpose(2,3).contiguous().view(-1, 21)\n","\n","            labels = labels_val.view(-1)\n","            val_loss = criterion(pred, labels)\n","            \n","\n","            preds_np = predictions.data.max(1)[1].cpu().numpy()\n","            labels_np = labels_val.data.cpu().numpy()\n","\n","            running_metrics.update(labels_np, preds_np)\n","            val_loss_avg.update(val_loss.item())\n","            \n","            running_val_loss += val_loss.item()\n","\n","            if iter_val % 50 == 0:\n","                print(\"{}/{} Loss: {}: \".format(iter_val, val_len, val_loss_avg.avg))\n","                val_loss_avg.reset()\n","\n","    score, class_iou = running_metrics.get_scores()\n","    \n","    print('\\n Avg train loss: {:.5f} vs Avg val loss: {:.5f}'\n","          .format(running_loss/len(traincrf_loader), running_val_loss/len(val_loader)))\n","\n","    print('\\nEpoch: {} Validation Summary'.format(actual_epoch))\n","    for k, v in score.items():\n","        print(k, v)\n","    #   writer.add_scalar('val_metrics/{}'.format(k), v, i+1)\n","\n","    running_metrics.reset()\n","\n","    if score[\"Mean IoU : \\t\"] >= best_iou:\n","        best_iou = score[\"Mean IoU : \\t\"]\n","        logger.info('Found new best_iou: {}'.format(best_iou))\n","        state = {\n","                \"epoch\": actual_epoch,\n","                \"model_state\": model.state_dict(),\n","                \"optimizer_state\": optimizer.state_dict(),\n","                \"best_iou\": best_iou,\n","                }\n","        logger.info(save_path)\n","        torch.save(state, save_path)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Wh6f9F46UILe","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]}]}